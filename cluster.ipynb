{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>留言编号</th>\n",
       "      <th>留言用户</th>\n",
       "      <th>留言主题</th>\n",
       "      <th>留言时间</th>\n",
       "      <th>留言详情</th>\n",
       "      <th>一级标签</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>A00074011</td>\n",
       "      <td>A市西湖建筑集团占道施工有安全隐患</td>\n",
       "      <td>2020/1/6 12:09:38</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tA3区大道西行便道，未管所路口至加油站路段，...</td>\n",
       "      <td>城乡建设</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>U0008473</td>\n",
       "      <td>A市在水一方大厦人为烂尾多年，安全隐患严重</td>\n",
       "      <td>2020/1/4 11:17:46</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t位于书院路主干道的在水一方大厦一楼至四楼人为...</td>\n",
       "      <td>城乡建设</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>A00063999</td>\n",
       "      <td>投诉A市A1区苑物业违规收停车费</td>\n",
       "      <td>2019/12/30 17:06:14</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t尊敬的领导：A1区苑小区位于A1区火炬路，小...</td>\n",
       "      <td>城乡建设</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>U0007137</td>\n",
       "      <td>A1区蔡锷南路A2区华庭楼顶水箱长年不洗</td>\n",
       "      <td>2019/12/6 14:40:14</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tA1区A2区华庭小区高层为二次供水，楼顶水箱...</td>\n",
       "      <td>城乡建设</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319</td>\n",
       "      <td>U0007137</td>\n",
       "      <td>A1区A2区华庭自来水好大一股霉味</td>\n",
       "      <td>2019/12/5 11:17:22</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tA1区A2区华庭小区高层为二次供水，楼顶水箱...</td>\n",
       "      <td>城乡建设</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   留言编号       留言用户                   留言主题                 留言时间  \\\n",
       "0    24  A00074011      A市西湖建筑集团占道施工有安全隐患    2020/1/6 12:09:38   \n",
       "1    37   U0008473  A市在水一方大厦人为烂尾多年，安全隐患严重    2020/1/4 11:17:46   \n",
       "2    83  A00063999       投诉A市A1区苑物业违规收停车费  2019/12/30 17:06:14   \n",
       "3   303   U0007137   A1区蔡锷南路A2区华庭楼顶水箱长年不洗   2019/12/6 14:40:14   \n",
       "4   319   U0007137      A1区A2区华庭自来水好大一股霉味   2019/12/5 11:17:22   \n",
       "\n",
       "                                                留言详情  一级标签  \n",
       "0  \\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tA3区大道西行便道，未管所路口至加油站路段，...  城乡建设  \n",
       "1  \\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t位于书院路主干道的在水一方大厦一楼至四楼人为...  城乡建设  \n",
       "2  \\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t尊敬的领导：A1区苑小区位于A1区火炬路，小...  城乡建设  \n",
       "3  \\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tA1区A2区华庭小区高层为二次供水，楼顶水箱...  城乡建设  \n",
       "4  \\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tA1区A2区华庭小区高层为二次供水，楼顶水箱...  城乡建设  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import jieba\n",
    "data = pd.read_csv('E:/taidibei/data/fujian2.csv',encoding = 'utf8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load F:/githubclass/HotNewsAnalysis/hot_news_analysis/utils/news_pandas.py\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "''''\n",
    "def save_news(news_df, path):\n",
    "    \"\"\"保存新闻\"\"\"\n",
    "    news_df.to_csv(path, index=False, encoding='utf-8')\n",
    "'''\n",
    "'''\n",
    "def replace_line_terminator(x):\n",
    "    \"\"\"替换行终止符\"\"\"\n",
    "    try:\n",
    "        x = re.sub(r'\\r\\n', '\\n', x)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return x\n",
    "'''\n",
    "\n",
    "def load_news(path):\n",
    "    \"\"\"加载新闻\"\"\"\n",
    "    news_df = pd.read_csv(path, encoding='utf-8')\n",
    "    news_df = news_df.applymap(replace_line_terminator)\n",
    "    return news_df\n",
    "\n",
    "\n",
    "def save_text(document, path):\n",
    "    \"\"\"保存txt文件\"\"\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(document)\n",
    "\n",
    "\n",
    "def load_text(path):\n",
    "    \"\"\"加载txt文件\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        document = f.read()\n",
    "    return document\n",
    "\n",
    "\n",
    "def save_element(element, path):\n",
    "    \"\"\"保存元素\"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(element, f)\n",
    "\n",
    "\n",
    "def load_element(path):\n",
    "    \"\"\"加载元素\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        element = pickle.load(f)\n",
    "    return element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load F:/githubclass/HotNewsAnalysis/hot_news_analysis/utils/ py\n",
    "\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "'''\n",
    "def data_filter(df):\n",
    "    \"\"\"数据过滤\"\"\"\n",
    "    # 过滤掉没有内容的新闻\n",
    "    df = df[df['content'] != ''].copy()\n",
    "    df = df.dropna(subset=['content']).copy()\n",
    "    # 去重\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "    df = df.drop_duplicates(subset=['title'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "'''\n",
    "\n",
    "def get_data(df, last_time, delta):\n",
    "    \"\"\"\n",
    "    获取某段时间的新闻数据\n",
    "    :param df: 原始数据\n",
    "    :param last_time: 指定要获取数据的最后时间\n",
    "    :param delta: 时间间隔\n",
    "    :return: last_time前timedelta的数据\n",
    "    \"\"\"\n",
    "    last_time = datetime.strptime(last_time, '%Y/%m/%d %H:%M:%S')\n",
    "    delta = timedelta(delta)\n",
    "    try:\n",
    "        df['留言时间'] = df['留言时间'].map(lambda x: datetime.strptime(x, '%Y/%m/%d %H:%M:%S'))\n",
    "    except TypeError:\n",
    "        pass\n",
    "    df = df[df['留言时间'].map(lambda x: (x <= last_time) and (x > last_time - delta))].copy()\n",
    "    print('df.shape=', df.shape)\n",
    "    if df.shape[0] == 0:\n",
    "        print('No Data!')\n",
    "        return df\n",
    "    df = df.sort_values(by=['留言时间'], ascending=[0])\n",
    "    df['留言时间'] = df['留言时间'].map(lambda x: datetime.strftime(x, '%Y/%m/%d %H:%M:%S'))\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "'''\n",
    "def clean_title_blank(title):\n",
    "    \"\"\"清理新闻标题空白\"\"\"\n",
    "    # 清理未知字符\n",
    "    title = re.sub(r'\\?+', ' ', title)\n",
    "    # 清理空白字符\n",
    "    title = re.sub(r'\\u3000', '', title)\n",
    "    title = title.strip()\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    title = re.sub(r'([|：])+ ', r'\\1', title)\n",
    "    title = re.sub(r' ([|：])+', r'\\1', title)\n",
    "    return title\n",
    "'''\n",
    "\n",
    "'''\n",
    "def clean_content_blank(content):\n",
    "    \"\"\"清理新闻内容空白\"\"\"\n",
    "    # 清理未知字符\n",
    "    content = re.sub(r'\\?+', ' ', content)\n",
    "    # 清理空白字符\n",
    "    content = re.sub(r'\\u3000', '', content)\n",
    "    content = content.strip()\n",
    "    content = re.sub(r'[ \\t\\r\\f]+', ' ', content)\n",
    "    content = re.sub(r'\\n ', '\\n', content)\n",
    "    content = re.sub(r' \\n', '\\n', content)\n",
    "    content = re.sub(r'\\n+', '\\n', content)\n",
    "    return content\n",
    "'''\n",
    "'''\n",
    "def clean_content(content):\n",
    "    \"\"\"清理新闻内容\"\"\"\n",
    "    # 清理新闻内容空白\n",
    "    content = clean_content_blank(content)\n",
    "    # 英文大写转小写\n",
    "    content = content.lower()\n",
    "    # 清理超链接\n",
    "    content = re.sub(r'https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]', '', content)\n",
    "    # 清理责任编辑等\n",
    "    content = re.split(r'\\n责任编辑', content)[0]\n",
    "    content = re.split(r'返回搜狐，查看更多', content)[0]\n",
    "    # 清理原标题\n",
    "    content = re.sub(r'原标题：.*\\n', '', content)\n",
    "    # 清理来源等和内容无关的文字\n",
    "    texts = [\n",
    "        r'新浪财经讯[ ，]*', r'新浪美股讯[ ，]*', r'新浪外汇讯[ ，]*', r'新浪科技讯[ ，]*',\n",
    "        r'[（\\(].{,10}来源[:：].{,30}[）\\)]',\n",
    "        r'(?<=\\n).{,2}来源[:：].{,30}\\n', r'(?<=\\n).{,2}来源[:：].{,30}$',\n",
    "        r'[（\\(].{,20}记者[ :：].{,20}[）\\)]',\n",
    "        r'(?<=\\n).{,2}作者：.{,20}\\n', r'(?<=\\n).{,2}作者：.{,20}$',\n",
    "        r'(?<=\\n).{,2}编辑：.{,20}\\n', r'(?<=\\n).{,2}编辑：.{,20}$'\n",
    "    ]\n",
    "    for text in texts:\n",
    "        content = re.sub(text, '', content)\n",
    "    content = re.sub(r'\\n+', '\\n', content)\n",
    "    return content\n",
    "'''\n",
    "\n",
    "def get_num_en_ch(text):\n",
    "    \"\"\"提取数字英文中文\"\"\"\n",
    "    text = re.sub(r'[^0-9A-Za-z\\u4E00-\\u9FFF]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def pseg_cut(text, userdict_path=None):\n",
    "    \"\"\"\n",
    "    词性标注\n",
    "    :param text: string，原文本数据\n",
    "    :param userdict_path: string，用户词词典路径，默认为None\n",
    "    :return: list， 分词后词性标注的列表\n",
    "    \"\"\"\n",
    "    if userdict_path is not None:\n",
    "        jieba.load_userdict(userdict_path)\n",
    "    words = pseg.lcut(text)\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_words_by_flags(words, flags=None):\n",
    "    \"\"\"\n",
    "    获取指定词性的词\n",
    "    :param words: list， 分词后词性标注的列表\n",
    "    :param flags: list， 词性标注，默认为提取名词和动词\n",
    "    :return: list， 指定词性的词\n",
    "    \"\"\"\n",
    "    flags = ['n.*', 'v.*'] if flags is None else flags\n",
    "    words = [w for w, f in words if w != ' ' and re.match('|'.join(['(%s$)' % flag for flag in flags]), f)]\n",
    "    return words\n",
    "\n",
    "\n",
    "def userdict_cut(text, userdict_path=None):\n",
    "    \"\"\"\n",
    "    对文本进行jieba分词\n",
    "    如果使用用户词词典，那么使用用户词词典进行jieba分词\n",
    "    \"\"\"\n",
    "    if userdict_path is not None:\n",
    "        jieba.load_userdict(userdict_path)\n",
    "    words = jieba.cut(text)\n",
    "    return words\n",
    "\n",
    "\n",
    "def stop_words_cut(words, stop_words_path):\n",
    "    \"\"\"停用词处理\"\"\"\n",
    "    with open(stop_words_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = [line.strip() for line in f.readlines()]\n",
    "        stopwords.append(' ')\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "    return words\n",
    "\n",
    "\n",
    "def disambiguation_cut(words, disambiguation_dict_path):\n",
    "    \"\"\"消歧词典\"\"\"\n",
    "    with open(disambiguation_dict_path, 'r', encoding='utf-8') as f:\n",
    "        disambiguation_dict = json.load(f)\n",
    "        words = [(disambiguation_dict[word]\n",
    "                  if disambiguation_dict.get(word) else word) for word in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def individual_character_cut(words, individual_character_dict_path):\n",
    "    \"\"\"删除无用单字\"\"\"\n",
    "    with open(individual_character_dict_path, 'r', encoding='utf-8') as f:\n",
    "        individual_character = [line.strip() for line in f.readlines()]\n",
    "        words = [word for word in words\n",
    "                 if ((len(word) > 1) or ((len(word) == 1) and (word in individual_character)))]\n",
    "    return words\n",
    "\n",
    "\n",
    "def document2txt(raw_document, userdict_path, text_path):\n",
    "    \"\"\"文本分词并保存为txt文件\"\"\"\n",
    "    document = clean_content_blank(raw_document)\n",
    "    document = document.lower()\n",
    "    document_cut = userdict_cut(document, userdict_path)\n",
    "    result = ' '.join(document_cut)\n",
    "    result = re.sub(r' +', ' ', result)\n",
    "    result = re.sub(r' \\n ', '\\n', result)\n",
    "    with open(text_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load F:/githubclass/HotNewsAnalysis/hot_news_analysis/utils/counter.py\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def flat(l):\n",
    "    \"\"\"平展多维列表\"\"\"\n",
    "    for k in l:\n",
    "        if not isinstance(k, (list, tuple)):\n",
    "            yield k\n",
    "        else:\n",
    "            yield from flat(k)\n",
    "\n",
    "\n",
    "def get_word_library(list1):\n",
    "    \"\"\"\n",
    "    获得词库\n",
    "    :param list1: 一维或多维词列表\n",
    "    :return: list，所有词去重之后的列表\n",
    "    \"\"\"\n",
    "    list2 = flat(list1)\n",
    "    list3 = list(set(list2))\n",
    "    return list3\n",
    "\n",
    "\n",
    "def get_single_frequency_words(list1):\n",
    "    \"\"\"\n",
    "    获得单频词列表\n",
    "    :param list1: 一维或多维词列表\n",
    "    :return: list，所有只出现一次的词组成的列表\n",
    "    \"\"\"\n",
    "    list2 = flat(list1)\n",
    "    cnt = Counter(list2)\n",
    "    list3 = [i for i in cnt if cnt[i] == 1]\n",
    "    return list3\n",
    "\n",
    "\n",
    "def get_most_common_words(list1, top_n=None, min_frequency=1):\n",
    "    \"\"\"\n",
    "    获取最常见的词组成的列表\n",
    "    :param list1: 一维或多维词列表\n",
    "    :param top_n: 指定最常见的前n个词，默认为None\n",
    "    :param min_frequency: 指定最小频数，默认为1\n",
    "    :return: list，最常见的前n个词组成的列表\n",
    "    \"\"\"\n",
    "    list2 = flat(list1)\n",
    "    cnt = Counter(list2)\n",
    "    list3 = [i[0] for i in cnt.most_common(top_n) if cnt[i[0]] >= min_frequency]\n",
    "    return list3\n",
    "\n",
    "\n",
    "def get_num_of_value_no_repeat(list1):\n",
    "    \"\"\"\n",
    "    获取列表中不重复的值的个数\n",
    "    :param list1: 列表\n",
    "    :return: int，列表中不重复的值的个数\n",
    "    \"\"\"\n",
    "    num = len(set(list1))\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load F:/githubclass/HotNewsAnalysis/hot_news_analysis/utils/modeling.py\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "from textrank4zh import TextRank4Sentence\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "def feature_extraction(series, vectorizer='CountVectorizer', vec_args=None):\n",
    "    \"\"\"\n",
    "    对原文本进行特征提取\n",
    "    :param series: pd.Series，原文本\n",
    "    :param vectorizer: string，矢量化器，如'CountVectorizer'或者'TfidfVectorizer'\n",
    "    :param vec_args: dict，矢量化器参数\n",
    "    :return: 稀疏矩阵\n",
    "    \"\"\"\n",
    "    vec_args = {'max_df': 1.0, 'min_df': 1} if vec_args is None else vec_args\n",
    "    vec_args_list = ['%s=%s' % (i[0],\n",
    "                                \"'%s'\" % i[1] if isinstance(i[1], str) else i[1]\n",
    "                                ) for i in vec_args.items()]\n",
    "    vec_args_str = ','.join(vec_args_list)\n",
    "    vectorizer1 = eval(\"%s(%s)\" % (vectorizer, vec_args_str))\n",
    "    matrix = vectorizer1.fit_transform(series)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_cluster(matrix, cluster='DBSCAN', cluster_args=None):\n",
    "    \"\"\"\n",
    "    对数据进行聚类，获取训练好的聚类器\n",
    "    :param matrix: 稀疏矩阵\n",
    "    :param cluster: string，聚类器\n",
    "    :param cluster_args: dict，聚类器参数\n",
    "    :return: 训练好的聚类器\n",
    "    \"\"\"\n",
    "    cluster_args = {'eps': 0.5, 'min_samples': 5, 'metric': 'cosine'} if cluster_args is None else cluster_args\n",
    "    cluster_args_list = ['%s=%s' % (i[0],\n",
    "                                    \"'%s'\" % i[1] if isinstance(i[1], str) else i[1]\n",
    "                                    ) for i in cluster_args.items()]\n",
    "    cluster_args_str = ','.join(cluster_args_list)\n",
    "    cluster1 = eval(\"%s(%s)\" % (cluster, cluster_args_str))\n",
    "    cluster1 = cluster1.fit(matrix)\n",
    "    return cluster1\n",
    "\n",
    "\n",
    "def get_labels(cluster):\n",
    "    \"\"\"\n",
    "    获取聚类标签\n",
    "    :param cluster: 训练好的聚类器\n",
    "    :return: list，聚类标签\n",
    "    \"\"\"\n",
    "    labels = cluster.labels_\n",
    "    return labels\n",
    "\n",
    "\n",
    "def label2rank(labels_list):\n",
    "    \"\"\"\n",
    "    按标签的数量将标签转换为排行\n",
    "    :param labels_list: list，聚类标签\n",
    "    :return: list，聚类排行\n",
    "    \"\"\"\n",
    "    series = pd.Series(labels_list)\n",
    "    list1 = series[series != -1].tolist()\n",
    "    n = len(set(list1))\n",
    "    cnt = Counter(list1)\n",
    "    key = [cnt.most_common()[i][0] for i in range(n)]\n",
    "    value = [i for i in range(1, n + 1)]\n",
    "    my_dict = dict(zip(key, value))\n",
    "    my_dict[-1] = -1\n",
    "    rank_list = [my_dict[i] for i in labels_list]\n",
    "    return rank_list\n",
    "\n",
    "\n",
    "def get_non_outliers_data(df, label_column='label'):\n",
    "    \"\"\"获取属于某个聚类簇的数据\"\"\"\n",
    "    df = df[df[label_column] != -1].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data_sort_labelnum(df, label_column='label', top=1):\n",
    "    \"\"\"\n",
    "    获取按标签数量排行的第top组数据\n",
    "    :param df: pd.DataFrame，带有标签列的数据\n",
    "    :param label_column: string，标签列名\n",
    "    :param top: int\n",
    "    :return: pd.DataFrame，前top组的数据\n",
    "    \"\"\"\n",
    "    assert top > 0, 'top不能小于等于0！'\n",
    "    labels = df[label_column].tolist()\n",
    "    cnt = Counter(labels)\n",
    "    label = cnt.most_common()[top - 1][0] if top <= len(set(labels)) else -2\n",
    "    df = df[df[label_column] == label].copy() if label != -2 else pd.DataFrame(columns=df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def list2wordcloud(list1, save_path, font_path):\n",
    "    \"\"\"\n",
    "    将文本做成词云\n",
    "    :param list1: list，文本列表\n",
    "    :param save_path: string，词云图片保存的路径\n",
    "    :param font_path: string，用于制作词云所需的字体路径\n",
    "    \"\"\"\n",
    "    text = ' '.join(list1)\n",
    "    wc = WordCloud(font_path=font_path, width=800, height=600, margin=2,\n",
    "                   ranks_only=True, max_words=200, collocations=False).generate(text)\n",
    "    wc.to_file(save_path)\n",
    "\n",
    "\n",
    "def get_key_sentences(text, num=1):\n",
    "    \"\"\"\n",
    "    利用textrank算法，获取文本摘要\n",
    "    :param text: string，原文本\n",
    "    :param num: int，指定摘要条数\n",
    "    :return: string，文本摘要\n",
    "    \"\"\"\n",
    "    tr4s = TextRank4Sentence(delimiters='\\n')\n",
    "    tr4s.analyze(text=text, lower=True, source='all_filters')\n",
    "    abstract = '\\n'.join([item.sentence for item in tr4s.get_key_sentences(num=num)])\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def feature_reduction(matrix, pca_n_components=50, tsne_n_components=2):\n",
    "    \"\"\"降维\"\"\"\n",
    "    data_pca = PCA(n_components=pca_n_components).fit_transform(matrix) if pca_n_components is not None else matrix\n",
    "    data_pca_tsne = TSNE(n_components=tsne_n_components).fit_transform(\n",
    "        data_pca) if tsne_n_components is not None else data_pca\n",
    "    print('data_pca_tsne.shape=', data_pca_tsne.shape)\n",
    "    return data_pca_tsne\n",
    "\n",
    "\n",
    "def get_word2vec_model(text_path):\n",
    "    \"\"\"训练词向量模型\"\"\"\n",
    "    sentences = word2vec.LineSentence(text_path)\n",
    "    model = word2vec.Word2Vec(sentences, hs=1, min_count=1, window=3, size=100)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_wordvec(model, word):\n",
    "    \"\"\"查询词是否在词库中\"\"\"\n",
    "    try:\n",
    "        model.wv.get_vector(word)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_word_and_wordvec(model, words):\n",
    "    \"\"\"获取输入词的词和对应的词向量\"\"\"\n",
    "    word_list = [i for i in words if get_wordvec(model, i)]\n",
    "    wordvec_list = [model.wv[i].tolist() for i in words if get_wordvec(model, i)]\n",
    "    return word_list, wordvec_list\n",
    "\n",
    "\n",
    "def get_top_words(words, label, label_num):\n",
    "    \"\"\"获得每个类中的前30个词\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['word'] = words\n",
    "    df['label'] = label\n",
    "    for i in range(label_num):\n",
    "        df_ = df[df['label'] == i]\n",
    "        print(df_['word'][:30])\n",
    "\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    \"\"\"保存模型\"\"\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"加载模型\"\"\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape= (0, 6)\n",
      "No Data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\a\\envs\\dxx2\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\a\\envs\\dxx2\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-47-24272a41ebbc>\", line 89, in title_cluster\n",
      "    vec_args={'max_df': 1.0, 'min_df': 1, 'max_features': max_features})\n",
      "  File \"<ipython-input-43-a579ec30c098>\", line 28, in feature_extraction\n",
      "    vectorizer1 = eval(\"%s(%s)\" % (vectorizer, vec_args_str))\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"d:\\a\\envs\\dxx2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 864, in __init__\n",
      "    % max_features)\n",
      "ValueError: max_features=0, neither a positive integer nor None\n",
      "\n",
      "Exception in thread Thread-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\a\\envs\\dxx2\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\a\\envs\\dxx2\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-47-24272a41ebbc>\", line 135, in content_cluster\n",
      "    vec_args={'max_df': 0.95, 'min_df': 1, 'max_features': max_features})\n",
      "  File \"<ipython-input-43-a579ec30c098>\", line 28, in feature_extraction\n",
      "    vectorizer1 = eval(\"%s(%s)\" % (vectorizer, vec_args_str))\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"d:\\a\\envs\\dxx2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 864, in __init__\n",
      "    % max_features)\n",
      "ValueError: max_features=0, neither a positive integer nor None\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'E:/taidibei/data\\\\results\\\\df_title_rank.csv' does not exist: b'E:/taidibei/data\\\\results\\\\df_title_rank.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-24272a41ebbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-24272a41ebbc>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mget_key_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-24272a41ebbc>\u001b[0m in \u001b[0;36mget_key_words\u001b[1;34m()\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_key_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mdf_title\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mload_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'df_title_rank.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[0mdf_content\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mload_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'df_content_rank.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mdf_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_cut'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_cut'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-24272a41ebbc>\u001b[0m in \u001b[0;36mload_news\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;34m\"\"\"加载新闻\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mnews_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m     \u001b[1;31m#news_df = news_df.applymap(replace_line_terminator)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\a\\envs\\dxx2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\a\\envs\\dxx2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\a\\envs\\dxx2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\a\\envs\\dxx2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\a\\envs\\dxx2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'E:/taidibei/data\\\\results\\\\df_title_rank.csv' does not exist: b'E:/taidibei/data\\\\results\\\\df_title_rank.csv'"
     ]
    }
   ],
   "source": [
    "# %load F:/githubclass/HotNewsAnalysis/hot_news_analysis/hot_news.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import threading\n",
    "\n",
    "# 获取项目路径\n",
    "#current_folder_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# 获取数据存放目录路径\n",
    "data_path = os.path.join('E:/taidibei/', 'data')\n",
    "fonts_path = os.path.join(data_path, 'fonts')\n",
    "images_path = os.path.join(data_path, 'images')\n",
    "texts_path = os.path.join(data_path, 'texts')\n",
    "extra_dict_path = os.path.join(data_path, 'extra_dict')\n",
    "models_path = os.path.join(data_path, 'models')\n",
    "news_path = os.path.join(data_path, 'news')\n",
    "temp_news_path = os.path.join(data_path, 'temp_news')\n",
    "results_path = os.path.join(data_path, 'results')\n",
    "\n",
    "'''\n",
    "def my_crawler():\n",
    "    \"\"\"爬取新闻数据\"\"\"\n",
    "    # sina_news_df =  get_latest_news('sina', top=1000, show_content=True)\n",
    "    # sohu_news_df =  get_latest_news('sohu', top=1000, show_content=True)\n",
    "    # xinhuanet_news_df =  get_latest_news('xinhuanet', top=100, show_content=True)\n",
    "    #  save_news(sina_news_df, os.path.join(news_path, 'sina_latest_news.csv'))\n",
    "    #  save_news(sohu_news_df, os.path.join(news_path, 'sohu_latest_news.csv'))\n",
    "    #  save_news(xinhuanet_news_df, os.path.join(news_path, 'xinhuanet_latest_news.csv'))\n",
    "    save_file_path = os.path.join(news_path, 'news_df.csv')\n",
    "    thread_crawler.threaded_crawler(1000, 1000, 10, save_file_path=save_file_path)\n",
    "'''\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"加载数据\"\"\"\n",
    "    # sina_news_df =  load_news(os.path.join(news_path, 'sample_sina_latest_news.csv'))\n",
    "    # sohu_news_df =  load_news(os.path.join(news_path, 'sample_sohu_latest_news.csv'))\n",
    "    # xinhuanet_news_df =  load_news(os.path.join(news_path, 'sample_xinhuanet_latest_news.csv'))\n",
    "    # sina_news_df =  load_news(os.path.join(news_path, 'sina_latest_news.csv'))\n",
    "    # sohu_news_df =  load_news(os.path.join(news_path, 'sohu_latest_news.csv'))\n",
    "    # xinhuanet_news_df =  load_news(os.path.join(news_path, 'xinhuanet_latest_news.csv'))\n",
    "    # news_df = pd.concat([sina_news_df, sohu_news_df, xinhuanet_news_df], ignore_index=True)\n",
    "    save_file_path = os.path.join(news_path, 'fujian2.csv')\n",
    "    news_df = load_news(save_file_path)\n",
    "    return news_df\n",
    "\n",
    "\n",
    "def filter_data(news_df):\n",
    "    \"\"\"过滤数据\"\"\"\n",
    "    #df = data_filter(news_df)\n",
    "    df = news_df\n",
    "    now_time = datetime.strftime(datetime.now(), '%Y/%m/%d %H:%M:%S')\n",
    "    # now_time = '2020-05-01 23:59'\n",
    "    df = get_data(df, last_time=now_time, delta=5)\n",
    "    return df\n",
    "\n",
    "\n",
    "def title_preprocess(df_title):\n",
    "    \"\"\"标题分词处理\"\"\"\n",
    "    df_title['title_'] = df_title['留言主题'].map(lambda x:  clean_title_blank(x))\n",
    "    df_title['title_'] = df_title['title_'].map(lambda x:  get_num_en_ch(x))\n",
    "    df_title['title_cut'] = df_title['title_'].map(lambda x:  pseg_cut(\n",
    "        x, userdict_path=os.path.join(extra_dict_path, 'self_userdict.txt')))\n",
    "    df_title['title_cut'] = df_title['title_cut'].map(lambda x:  get_words_by_flags(\n",
    "        x, flags=['n.*', '.*n', 'v.*', 's', 'j', 'l', 'i', 'eng']))\n",
    "    df_title['title_cut'] = df_title['title_cut'].map(lambda x:  stop_words_cut(\n",
    "        x, os.path.join(extra_dict_path, 'self_stop_words.txt')))\n",
    "    df_title['title_cut'] = df_title['title_cut'].map(lambda x:  disambiguation_cut(\n",
    "        x, os.path.join(extra_dict_path, 'self_disambiguation_dict.json')))\n",
    "    df_title['title_cut'] = df_title['title_cut'].map(lambda x:  individual_character_cut(\n",
    "        x, os.path.join(extra_dict_path, 'self_individual_character_dict.txt')))\n",
    "    df_title['title_'] = df_title['title_cut'].map(lambda x: ' '.join(x))\n",
    "    return df_title\n",
    "\n",
    "def save_news(news_df, path):\n",
    "    \"\"\"保存新闻\"\"\"\n",
    "    news_df.to_csv(path, index=False, encoding='utf-8')\n",
    "    \n",
    "\n",
    "def title_cluster(df, save_df=False):\n",
    "    \"\"\"按新闻标题聚类\"\"\"\n",
    "    df_title = df.copy()\n",
    "    df_title = title_preprocess(df_title)\n",
    "    word_library_list = get_word_library(df_title['title_cut'])\n",
    "    single_frequency_words_list = get_single_frequency_words(df_title['title_cut'])\n",
    "    max_features = len(word_library_list) - len(single_frequency_words_list) // 2\n",
    "    title_matrix = feature_extraction(df_title['title_'], vectorizer='CountVectorizer',\n",
    "                                               vec_args={'max_df': 1.0, 'min_df': 1, 'max_features': max_features})\n",
    "    title_dbscan = get_cluster(title_matrix, cluster='DBSCAN',\n",
    "                                        cluster_args={'eps': 0.4, 'min_samples': 4, 'metric': 'cosine'})\n",
    "    title_labels = get_labels(title_dbscan)\n",
    "    df_title['title_label'] = title_labels\n",
    "    df_non_outliers = get_non_outliers_data(df_title, label_column='title_label')\n",
    "    title_label_num = get_num_of_value_no_repeat(df_non_outliers['title_label'].tolist())\n",
    "    print('按新闻标题聚类，一共有%d个簇(不包括离群点)' % title_label_num)\n",
    "    title_rank = label2rank(title_labels)\n",
    "    df_title['title_rank'] = title_rank\n",
    "    for i in range(1, title_label_num + 1):\n",
    "        df_ = df_title[df_title['title_rank'] == i]\n",
    "        title_top_list = get_most_common_words(df_['title_cut'], top_n=10)\n",
    "        print(title_top_list)\n",
    "    if save_df:\n",
    "        df_title.drop(['留言详情', 'title_', 'title_label'], axis=1, inplace=True)\n",
    "        save_news(df_title, os.path.join(results_path, 'df_title_rank.csv'))\n",
    "    return df_title\n",
    "\n",
    "\n",
    "def content_preprocess(df_content):\n",
    "    \"\"\"新闻内容分词处理\"\"\"\n",
    "    df_content['content_'] = df_content['留言详情'].map(lambda x:  clean_content(x))\n",
    "    df_content['content_'] = df_content['content_'].map(lambda x:  get_num_en_ch(x))\n",
    "    df_content['content_cut'] = df_content['content_'].map(lambda x:  pseg_cut(\n",
    "        x, userdict_path=os.path.join(extra_dict_path, 'self_userdict.txt')))\n",
    "    df_content['content_cut'] = df_content['content_cut'].map(lambda x:  get_words_by_flags(\n",
    "        x, flags=['n.*', '.*n', 'v.*', 's', 'j', 'l', 'i', 'eng']))\n",
    "    df_content['content_cut'] = df_content['content_cut'].map(lambda x:  stop_words_cut(\n",
    "        x, os.path.join(extra_dict_path, 'self_stop_words.txt')))\n",
    "    df_content['content_cut'] = df_content['content_cut'].map(lambda x:  disambiguation_cut(\n",
    "        x, os.path.join(extra_dict_path, 'self_disambiguation_dict.json')))\n",
    "    df_content['content_cut'] = df_content['content_cut'].map(lambda x:  individual_character_cut(\n",
    "        x, os.path.join(extra_dict_path, 'self_individual_character_dict.txt')))\n",
    "    df_content['content_'] = df_content['content_cut'].map(lambda x: ' '.join(x))\n",
    "    return df_content\n",
    "\n",
    "\n",
    "def content_cluster(df, df_save=False):\n",
    "    \"\"\"按新闻内容聚类\"\"\"\n",
    "    df_content = df.copy()\n",
    "    df_content = content_preprocess(df_content)\n",
    "    word_library_list = get_word_library(df_content['content_cut'])\n",
    "    single_frequency_words_list = get_single_frequency_words(df_content['content_cut'])\n",
    "    max_features = len(word_library_list) - len(single_frequency_words_list) // 2\n",
    "    content_matrix = feature_extraction(df_content['content_'], vectorizer='CountVectorizer',\n",
    "                                                 vec_args={'max_df': 0.95, 'min_df': 1, 'max_features': max_features})\n",
    "    content_dbscan = get_cluster(content_matrix, cluster='DBSCAN',\n",
    "                                          cluster_args={'eps': 0.35, 'min_samples': 4, 'metric': 'cosine'})\n",
    "    content_labels = get_labels(content_dbscan)\n",
    "    df_content['content_label'] = content_labels\n",
    "    df_non_outliers = get_non_outliers_data(df_content, label_column='content_label')\n",
    "    content_label_num =  get_num_of_value_no_repeat(df_non_outliers['content_label'].tolist())\n",
    "    print('按新闻内容聚类，一共有%d个簇(不包括离群点)' % content_label_num)\n",
    "    content_rank =  label2rank(content_labels)\n",
    "    df_content['content_rank'] = content_rank\n",
    "    for i in range(1, content_label_num + 1):\n",
    "        df_ = df_content[df_content['content_rank'] == i]\n",
    "        content_top_list =  get_most_common_words(df_['content_cut'], top_n=15, min_frequency=1)\n",
    "        print(content_top_list)\n",
    "    if df_save:\n",
    "        df_content.drop(['content_', 'content_label'], axis=1, inplace=True)\n",
    "        save_news(df_content, os.path.join(results_path, 'df_content_rank.csv'))\n",
    "    return df_content\n",
    "\n",
    "\n",
    "def get_wordcloud(df, rank_column, text_list_column):\n",
    "    \"\"\"\n",
    "    按照不同的簇生成每个簇的词云\n",
    "    :param df: pd.DataFrame，带有排名和分词后的文本列表数据\n",
    "    :param rank_column: 排名列名\n",
    "    :param text_list_column: 分词后的文本列表列名\n",
    "    \"\"\"\n",
    "    df_non_outliers =  get_non_outliers_data(df, label_column=rank_column)\n",
    "    label_num =  get_num_of_value_no_repeat(df_non_outliers[rank_column].tolist())\n",
    "    wordcloud_folder_path = os.path.join(results_path, rank_column)\n",
    "    if not os.path.exists(wordcloud_folder_path):\n",
    "        os.mkdir(wordcloud_folder_path)\n",
    "    for i in range(1, label_num + 1):\n",
    "        df_ = df[df[rank_column] == i]\n",
    "        list_ =  flat(df_[text_list_column].tolist())\n",
    "        list2wordcloud(list_, save_path=os.path.join(wordcloud_folder_path, '%d.png' % i),\n",
    "                                font_path=os.path.join(fonts_path, 'simhei.ttf'))\n",
    "\n",
    "\n",
    "def key_content(df, df_save=False):\n",
    "    \"\"\"获取摘要\"\"\"\n",
    "\n",
    "    def f(text):\n",
    "        text =  clean_content(text)\n",
    "        text =  get_key_sentences(text, num=1)\n",
    "        return text\n",
    "\n",
    "    df['abstract'] = df['留言详情'].map(f)\n",
    "    if df_save:\n",
    "        df.drop(['留言详情'], axis=1, inplace=True)\n",
    "        save_news(df, os.path.join(results_path, 'df_abstract.csv'))\n",
    "    return df\n",
    "\n",
    "def load_news(path):\n",
    "    \"\"\"加载新闻\"\"\"\n",
    "    news_df = pd.read_csv(path, encoding='utf-8')\n",
    "    #news_df = news_df.applymap(replace_line_terminator)\n",
    "    return news_df\n",
    "\n",
    "def get_key_words():\n",
    "    df_title =  load_news(os.path.join(results_path, 'df_title_rank.csv'))\n",
    "    df_content =  load_news(os.path.join(results_path, 'df_content_rank.csv'))\n",
    "    df_title['title_cut'] = df_title['title_cut'].map(eval)\n",
    "    df_content['content_cut'] = df_content['content_cut'].map(eval)\n",
    "    get_wordcloud(df_content, 'content_rank', 'content_cut')\n",
    "    df_title_content = df_title.copy()\n",
    "    df_title_content['content_cut'] = df_content['content_cut']\n",
    "    df_title_content['content_rank'] = df_content['content_rank']\n",
    "    df_title_content =  get_non_outliers_data(df_title_content, label_column='title_rank')\n",
    "    title_rank_num =  get_num_of_value_no_repeat((df_title_content['title_rank']))\n",
    "    for i in range(1, title_rank_num + 1):\n",
    "        df_i = df_title_content[df_title_content['title_rank'] == i]\n",
    "        title = '\\n'.join(df_i['留言主题'].tolist())\n",
    "        title =  get_key_sentences(title, num=1)\n",
    "        print('热点：', title)\n",
    "        content_rank = [k for k in df_i['content_rank']]\n",
    "        content_rank = set(content_rank)\n",
    "        for j in content_rank:\n",
    "            df_j = df_i[df_i['content_rank'] == j]\n",
    "            most_commmon_words =  get_most_common_words(df_j['content_cut'], top_n=20, min_frequency=5)\n",
    "            if len(most_commmon_words) > 0:\n",
    "                print('相关词汇：', most_commmon_words)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # # my_crawler()\n",
    "    news_df = load_data()\n",
    "    df = filter_data(news_df)\n",
    "    # title_cluster(df, True)\n",
    "    # content_cluster(df, True)\n",
    "    t1 = threading.Thread(target=title_cluster, args=(df, True))\n",
    "    t2 = threading.Thread(target=content_cluster, args=(df, True))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    threads = [t1, t2]\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    get_key_words()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/taidibei/data\\results\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('E:/taidibei/', 'data')\n",
    "results_path = os.path.join(data_path, 'results')\n",
    "\n",
    "print(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_library_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-92f61cc59a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_library_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_library_list' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
